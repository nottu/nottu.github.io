<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Javier Peralta</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2019-03-28T01:08:10-06:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Javier Peralta</name>
   <email></email>
 </author>

 
 <entry>
   <title>Exploiting Cyclic Symmetry in Convolutional Neural Networks</title>
   <link href="http://localhost:4000/2019/03/27/cyclic-symmetry/"/>
   <updated>2019-03-27T00:00:00-06:00</updated>
   <id>http://localhost:4000/2019/03/27/cyclic-symmetry</id>
   <content type="html">&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1602.02660&quot;&gt;Source&lt;/a&gt;&lt;/p&gt;

&lt;table class=&quot;authors&quot;&gt;
  &lt;tr&gt;
    &lt;td&gt;Sander Dielman&lt;/td&gt;
    &lt;td class=&quot;email&quot;&gt;&lt;a href=&quot;mailto:SEDIELEM@GOOGLE.COM&quot;&gt;SEDIELEM@GOOGLE.COM&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Jeffrey De Fauw&lt;/td&gt;
    &lt;td class=&quot;email&quot;&gt;&lt;a href=&quot;mailto:DEFAUW@GOOGLE.COM&quot;&gt;DEFAUW@GOOGLE.COM&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Koray Kavukcuoglu&lt;/td&gt;
    &lt;td class=&quot;email&quot;&gt;&lt;a href=&quot;mailto:KORAYK@GOOGLE.COM&quot;&gt;KORAYK@GOOGLE.COM&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;!-- In this paper the use of four new operations that are aimed to introduce rotational equivariance in **convolutional neural networks** (*CNNs*) is discussed. --&gt;

&lt;p&gt;The authors of this paper propose four new neural networks, that can be used together to build &lt;strong&gt;CNNs&lt;/strong&gt; (Convolutional Neural Networks) that are partially or fully rotation equivariant.&lt;/p&gt;

&lt;h3 id=&quot;cyclic-symmetry&quot;&gt;Cyclic Symmetry&lt;/h3&gt;

&lt;p&gt;CNN will often learn multiple copies of the same filter at different orientations. Encoding rotational symmetry into the architecture of the network can reduce the redundancy of learning patterns at different orientations, and can reduce the number of parameters, diminishing the risk of over-fitting.&lt;/p&gt;

&lt;p&gt;There are only four possible rotations of the input that would not require interpolation over angles $ k \cdot 90^\text{o}, k \in [0, 1, 2, 3]$. This restricted form of restricted symmetry is what is called &lt;strong&gt;cyclic symmetry&lt;/strong&gt; by the authors of the paper. Applying a horizontal flip, we can get eight possible orientations: &lt;strong&gt;dihedral symmetry&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Cyclic symmetry&lt;/em&gt; can be encoded in CNNs by parameter sharing.&lt;/p&gt;

&lt;h3 id=&quot;equivariance-and-invariance&quot;&gt;Equivariance and Invariance&lt;/h3&gt;

&lt;p&gt;A function $f(x)$ is called equivariant to a class of transformations $T$, if&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(tx) = t^\prime f(x) \,\,\,\, \forall t \in T&lt;/script&gt;

&lt;p&gt;where $t^\prime$ is some transformation of the output. That is, transforming an input $x$ and then passing it to $f$ gives the same result as passing $x$ to $f$ and then transforming the output. Furthermore, a function $f$ is invariant to this class of transformations if&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(tx) = f(x) \,\,\,\, \forall t \in T&lt;/script&gt;

</content>
 </entry>
 
 <entry>
   <title>Shape Analysis?</title>
   <link href="http://localhost:4000/2019/03/27/shape-analysis-intro/"/>
   <updated>2019-03-27T00:00:00-06:00</updated>
   <id>http://localhost:4000/2019/03/27/shape-analysis-intro</id>
   <content type="html">&lt;h2 id=&quot;what-is-a-shape&quot;&gt;What is a shape?&lt;/h2&gt;

</content>
 </entry>
 

</feed>
